{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T14:25:27.691511Z",
     "start_time": "2025-06-20T14:25:27.680734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "# For GPU support, install the following packages:\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "!pip install gymnasium[box2d]\n",
    "'''"
   ],
   "id": "c2d1e5eed6eabfc8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\\n!pip install gymnasium[box2d]\\n!pip install stable-baselines3[extra]\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T14:25:32.917005Z",
     "start_time": "2025-06-20T14:25:30.211963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import gymnasium as gym\n",
    "import itertools\n",
    "import time"
   ],
   "id": "9ee3d51a72b43fd1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T14:25:35.782616Z",
     "start_time": "2025-06-20T14:25:34.928995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# check if GPU is available\n",
    "torch.cuda.is_available()"
   ],
   "id": "c9ee60693d6085a0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T14:25:40.503383Z",
     "start_time": "2025-06-20T14:25:40.010332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env = gym.make(\"LunarLanderContinuous-v3\")\n",
    "\n",
    "print(\"State shape:\", env.observation_space.shape)\n",
    "state_size = env.observation_space.shape[0]\n",
    "print(\"State size:\", state_size)\n",
    "print(\"Number of continuous actions:\", env.action_space.shape[0])"
   ],
   "id": "9342159ab257b54b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape: (8,)\n",
      "State size: 8\n",
      "Number of continuous actions: 2\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T14:25:54.802895Z",
     "start_time": "2025-06-20T14:25:54.796098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "main_bins = [-1, 1]\n",
    "lateral_bins = [-1, 0, 1]\n",
    "\n",
    "discrete_actions = np.array(list(itertools.product(main_bins, lateral_bins)))\n",
    "number_actions = len(discrete_actions)"
   ],
   "id": "1ab09d2a9b016cf7",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T14:25:56.734352Z",
     "start_time": "2025-06-20T14:25:56.729141Z"
    }
   },
   "cell_type": "code",
   "source": "discrete_actions",
   "id": "4c2b4eb12785ecd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -1],\n",
       "       [-1,  0],\n",
       "       [-1,  1],\n",
       "       [ 1, -1],\n",
       "       [ 1,  0],\n",
       "       [ 1,  1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T14:26:48.224641Z",
     "start_time": "2025-06-20T14:26:48.216751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "learning_rate = 5e-4\n",
    "minibatch_size = 64\n",
    "discount_factor = 0.99\n",
    "replay_buffer_size = int(1e5)\n",
    "interpolation_parameter = 1e-3"
   ],
   "id": "c172c57412ab55ea",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T14:26:51.237878Z",
     "start_time": "2025-06-20T14:26:51.230250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Network(nn.Module):\n",
    "  def __init__(self, state_size, action_size, seed=42):\n",
    "      super(Network, self).__init__()\n",
    "      self.seed = torch.manual_seed(seed)\n",
    "      self.fc1 = nn.Linear(state_size, 128)\n",
    "      self.fc2 = nn.Linear(128, 128)\n",
    "      self.fc3 = nn.Linear(128, 64)\n",
    "      self.fc4 = nn.Linear(64, action_size)\n",
    "\n",
    "  def forward(self, state):\n",
    "      x = F.relu(self.fc1(state))\n",
    "      x = F.relu(self.fc2(x))\n",
    "      x = F.relu(self.fc3(x))\n",
    "      return self.fc4(x)  # Output Q-values for each discrete action"
   ],
   "id": "25e734793fa5354e",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T14:26:52.151941Z",
     "start_time": "2025-06-20T14:26:52.144132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "  def __init__(self, capacity):\n",
    "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    self.capacity = capacity\n",
    "    self.memory = []\n",
    "\n",
    "  def push(self, event):\n",
    "    self.memory.append(event)\n",
    "    if len(self.memory) > self.capacity:\n",
    "      del self.memory[0]\n",
    "\n",
    "  def sample(self, batch_size):\n",
    "    experiences = random.sample(self.memory, k = batch_size)\n",
    "    states = torch.from_numpy(np.vstack([e[0] for e in experiences if e is not None])).float().to(self.device)\n",
    "    actions = torch.from_numpy(np.vstack([e[1] for e in experiences if e is not None])).long().to(self.device)\n",
    "    rewards = torch.from_numpy(np.vstack([e[2] for e in experiences if e is not None])).float().to(self.device)\n",
    "    next_states = torch.from_numpy(np.vstack([e[3] for e in experiences if e is not None])).float().to(self.device)\n",
    "    dones = torch.from_numpy(np.vstack([e[4] for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
    "    return states, next_states, actions, rewards, dones"
   ],
   "id": "f04add999749c321",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T14:26:54.007350Z",
     "start_time": "2025-06-20T14:26:53.997298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Agent():\n",
    "\n",
    "  def __init__(self, state_size, action_size):\n",
    "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    self.state_size = state_size\n",
    "    self.action_size = action_size\n",
    "    self.discrete_actions = discrete_actions\n",
    "    self.local_qnetwork = Network(state_size, action_size).to(self.device)\n",
    "    self.target_qnetwork = Network(state_size, action_size).to(self.device)\n",
    "    self.optimizer = optim.Adam(self.local_qnetwork.parameters(), lr = learning_rate)\n",
    "    self.memory = ReplayMemory(replay_buffer_size)\n",
    "    self.t_step = 0\n",
    "\n",
    "  def step(self, state, action_idx, reward, next_state, done):\n",
    "    self.memory.push((state, action_idx, reward, next_state, done))\n",
    "    self.t_step = (self.t_step + 1) % 4\n",
    "    if self.t_step == 0:\n",
    "      if len(self.memory.memory) > minibatch_size:\n",
    "        experiences = self.memory.sample(minibatch_size)\n",
    "        self.learn(experiences, discount_factor)\n",
    "\n",
    "  def act(self, state, epsilon = 0.):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "    self.local_qnetwork.eval()\n",
    "    with torch.no_grad():\n",
    "      q_values = self.local_qnetwork(state).cpu().data.numpy().squeeze()\n",
    "    self.local_qnetwork.train()\n",
    "\n",
    "    if np.random.rand() < epsilon:\n",
    "        action_idx = np.random.randint(self.action_size)\n",
    "    else:\n",
    "        action_idx = np.argmax(q_values)\n",
    "    return action_idx  # Return index\n",
    "\n",
    "\n",
    "  def learn(self, experiences, discount_factor):\n",
    "    states, next_states, actions, rewards, dones = experiences\n",
    "    # For continuous actions, use MSE between predicted and taken actions (DDPG/A2C would be better)\n",
    "    actions = actions.long()\n",
    "    q_expected = self.local_qnetwork(states).gather(1, actions)\n",
    "    q_targets_next = self.target_qnetwork(next_states).max(1)[0].unsqueeze(1)\n",
    "    q_targets = rewards + discount_factor * q_targets_next * (1 - dones)\n",
    "    loss = F.mse_loss(q_expected, q_targets)\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "    self.soft_update(self.local_qnetwork, self.target_qnetwork, interpolation_parameter)\n",
    "\n",
    "  def soft_update(self, local_model, target_model, interpolation_parameter):\n",
    "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "      target_param.data.copy_(interpolation_parameter * local_param.data + (1.0 - interpolation_parameter) * target_param.data)"
   ],
   "id": "e24f6481e9df9868",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T14:27:05.024017Z",
     "start_time": "2025-06-20T14:27:02.248574Z"
    }
   },
   "cell_type": "code",
   "source": "agent = Agent(state_size, number_actions)",
   "id": "60a743baf473e3ae",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T09:46:09.451555Z",
     "start_time": "2025-06-08T09:35:02.110394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "number_episodes = 2000\n",
    "maximum_number_timesteps_per_episode = 1000\n",
    "epsilon_starting_value  = 1.0\n",
    "epsilon_ending_value  = 0.01\n",
    "epsilon_decay_value  = 0.996\n",
    "epsilon = epsilon_starting_value\n",
    "scores_on_100_episodes = deque(maxlen = 100)\n",
    "\n",
    "for episode in range(1, number_episodes + 1):\n",
    "  state, _ = env.reset()\n",
    "  score = 0\n",
    "  for t in range(maximum_number_timesteps_per_episode):\n",
    "    action_idx = agent.act(state, epsilon)\n",
    "    action = discrete_actions[action_idx]\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "    agent.step(state, action_idx, reward, next_state, done)\n",
    "    state = next_state\n",
    "    score += reward\n",
    "    if done:\n",
    "      break\n",
    "  scores_on_100_episodes.append(score)\n",
    "  epsilon = max(epsilon_ending_value, epsilon_decay_value * epsilon)\n",
    "  print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)), end = \"\")\n",
    "  if episode % 100 == 0:\n",
    "    print('\\rEpisode {}\\tAverage Score: {:.2f}\\tEpsilon: {:.2f}'.format(episode, np.mean(scores_on_100_episodes), epsilon))\n",
    "\n",
    "  if np.mean(scores_on_100_episodes) >= 200.0:\n",
    "    print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(episode - 100, np.mean(scores_on_100_episodes)))\n",
    "    torch.save(agent.local_qnetwork.state_dict(), 'checkpoint_stage_1.pth')\n",
    "    break\n"
   ],
   "id": "643d376a7d127743",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -282.29\tEpsilon: 0.67\n",
      "Episode 200\tAverage Score: -208.71\tEpsilon: 0.45\n",
      "Episode 300\tAverage Score: -54.08\tEpsilon: 0.30\n",
      "Episode 400\tAverage Score: 29.00\tEpsilon: 0.20\n",
      "Episode 500\tAverage Score: 123.98\tEpsilon: 0.13\n",
      "Episode 600\tAverage Score: 183.18\tEpsilon: 0.09\n",
      "Episode 700\tAverage Score: 194.01\tEpsilon: 0.06\n",
      "Episode 800\tAverage Score: 182.95\tEpsilon: 0.04\n",
      "Episode 828\tAverage Score: 201.32\n",
      "Environment solved in 728 episodes!\tAverage Score: 201.32\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T09:54:08.300705Z",
     "start_time": "2025-06-08T09:46:09.506073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the trained model weights\n",
    "agent.local_qnetwork.load_state_dict(torch.load('checkpoint_stage_1.pth', weights_only=True))\n",
    "agent.target_qnetwork.load_state_dict(torch.load('checkpoint_stage_1.pth', weights_only=True))\n",
    "\n",
    "# Now continue training as before\n",
    "number_episodes = 2000\n",
    "maximum_number_timesteps_per_episode = 1000\n",
    "epsilon_starting_value  = 1.0\n",
    "epsilon_ending_value  = 0.01\n",
    "epsilon_decay_value  = 0.996\n",
    "epsilon = epsilon_starting_value\n",
    "scores_on_100_episodes = deque(maxlen = 100)\n",
    "\n",
    "for episode in range(1, number_episodes + 1):\n",
    "    state, _ = env.reset()\n",
    "    score = 0\n",
    "    for t in range(maximum_number_timesteps_per_episode):\n",
    "        action_idx = agent.act(state, epsilon)\n",
    "        action = discrete_actions[action_idx]\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "\n",
    "        distance_from_pad = abs(next_state[0])\n",
    "        reward -= distance_from_pad * 2  # Penalize being far from the center\n",
    "\n",
    "        reward -= 0.01  # Gentle time penalty\n",
    "\n",
    "\n",
    "        agent.step(state, action_idx, reward, next_state, done)\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        if done:\n",
    "            break\n",
    "    scores_on_100_episodes.append(score)\n",
    "    epsilon = max(epsilon_ending_value, epsilon_decay_value * epsilon)\n",
    "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)), end = \"\")\n",
    "    if episode % 100 == 0:\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tEpsilon: {:.2f}'.format(episode, np.mean(scores_on_100_episodes), epsilon))\n",
    "    if np.mean(scores_on_100_episodes) >= 200.0:\n",
    "        print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(episode - 100, np.mean(scores_on_100_episodes)))\n",
    "        torch.save(agent.local_qnetwork.state_dict(), 'checkpoint_stage_2.pth')\n",
    "        break"
   ],
   "id": "6e4d5c254c372bb6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -262.91\tEpsilon: 0.67\n",
      "Episode 200\tAverage Score: -309.29\tEpsilon: 0.45\n",
      "Episode 300\tAverage Score: -210.65\tEpsilon: 0.30\n",
      "Episode 400\tAverage Score: -152.91\tEpsilon: 0.20\n",
      "Episode 500\tAverage Score: -95.09\tEpsilon: 0.13\n",
      "Episode 600\tAverage Score: 91.99\tEpsilon: 0.09\n",
      "Episode 700\tAverage Score: 165.41\tEpsilon: 0.06\n",
      "Episode 744\tAverage Score: 201.38\n",
      "Environment solved in 644 episodes!\tAverage Score: 201.38\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T14:27:14.387022Z",
     "start_time": "2025-06-20T14:27:14.380495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def play_with_pygame(agent, env_name, discrete_actions, delay=0.03):\n",
    "    env = gym.make(env_name, render_mode=\"human\", gravity=-10.0,\n",
    "               enable_wind=True, wind_power=15.0, turbulence_power=1.5)\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action_idx = agent.act(state, epsilon=0.0)  # Greedy action\n",
    "        action = discrete_actions[action_idx]\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        env.render()\n",
    "        time.sleep(delay)\n",
    "    env.close()"
   ],
   "id": "b90fbd8cab121db8",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T14:27:16.779419Z",
     "start_time": "2025-06-20T14:27:16.745720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Instantiate agent\n",
    "agent = Agent(state_size, number_actions)\n",
    "\n",
    "# Load the trained weights\n",
    "agent.local_qnetwork.load_state_dict(torch.load('checkpoint_stage_2.pth', weights_only=True))\n",
    "agent.target_qnetwork.load_state_dict(torch.load('checkpoint_stage_2.pth', weights_only=True))"
   ],
   "id": "bd547f788b984a67",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T14:27:39.158499Z",
     "start_time": "2025-06-20T14:27:17.866046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Play using the trained agent\n",
    "play_with_pygame(agent, \"LunarLanderContinuous-v3\", discrete_actions, delay=0.03)"
   ],
   "id": "ccd850999702b554",
   "outputs": [],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
